{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7k7QhHtqLVc"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqhRdHds4mRC"
      },
      "outputs": [],
      "source": [
        "# Google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjmMIejP5SC5"
      },
      "outputs": [],
      "source": [
        "# Train set\n",
        "!unzip drive/My\\ Drive/ESA/FullDataset.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AErE2s5CWzfW"
      },
      "outputs": [],
      "source": [
        "# First test set\n",
        "!unzip drive/My\\ Drive/ESA/TestData2.zip "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test set\n",
        "!unzip drive/My\\ Drive/ESA/final_evaluation_set.zip "
      ],
      "metadata": {
        "id": "KphgF9_n7Odp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6FJxBmaW8tP"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdYjx8tarDaz"
      },
      "outputs": [],
      "source": [
        "!pip install pybaselines\n",
        "!pip install pot\n",
        "!pip install nestle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9eO6F4TIHQt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ot\n",
        "import math\n",
        "import h5py\n",
        "import scipy\n",
        "import nestle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pybaselines\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pybaselines import utils\n",
        "from sklearn import preprocessing\n",
        "from matplotlib.cbook import delete_masked_points\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWgpEVrmqOtv"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9bVMcNbknqB"
      },
      "source": [
        "*Your code*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdY6NMwXW8do"
      },
      "outputs": [],
      "source": [
        "def to_observed_matrix(data_file,aux_file):\n",
        "    # careful, orders in data files are scambled. We need to \"align them with id from aux file\"\n",
        "    num = len(data_file.keys())\n",
        "    id_order = aux_file['planet_ID'].to_numpy()\n",
        "    observed_spectrum = np.zeros((num,52,4))\n",
        "\n",
        "    for idx, x in tqdm(enumerate(id_order), total=num):\n",
        "        current_planet_id = f'Planet_{x}'\n",
        "        instrument_wlgrid = data_file[current_planet_id]['instrument_wlgrid'][:]\n",
        "        instrument_spectrum = data_file[current_planet_id]['instrument_spectrum'][:]\n",
        "        instrument_noise = data_file[current_planet_id]['instrument_noise'][:]\n",
        "        instrument_wlwidth = data_file[current_planet_id]['instrument_width'][:]\n",
        "        observed_spectrum[idx,:,:] = np.concatenate([instrument_wlgrid[...,np.newaxis],\n",
        "                                            instrument_spectrum[...,np.newaxis],\n",
        "                                            instrument_noise[...,np.newaxis],\n",
        "                                            instrument_wlwidth[...,np.newaxis]],axis=-1)\n",
        "    return observed_spectrum\n",
        "\n",
        "\n",
        "def standardise(arr, mean, std):\n",
        "    return (arr-mean)/std\n",
        "\n",
        "def transform_back(arr, mean, std):\n",
        "    return arr*std+mean\n",
        "\n",
        "def augment_data(arr, noise, repeat=10):\n",
        "    noise_profile = np.random.normal(loc=0, scale=noise, size=(repeat,arr.shape[0], arr.shape[1]))\n",
        "    #noise_profile = np.random.uniform(low=0, high=noise, size=(repeat,arr.shape[0], arr.shape[1]))\n",
        "    ## produce noised version of the spectra\n",
        "    aug_arr = arr[np.newaxis, ...] + noise_profile\n",
        "    return aug_arr, noise_profile\n",
        "\n",
        "def visualise_spectrum(spectrum):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    plt.errorbar(x=spectrum[:,0], y= spectrum[:,1], yerr=spectrum[:,2] )\n",
        "    ## usually we visualise it in log-scale\n",
        "    plt.xscale('log')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MieI-DqoewRO"
      },
      "outputs": [],
      "source": [
        "def light_track_metric(targets, predictions, k =100):\n",
        "    \"\"\"\n",
        "    RMSE based Metric for light track. Compare quartiles between MCMC-based methods and model output\"\n",
        "    targets: The reference quartiles generated from a MCMC technique (N x 3 x num_targets,)\n",
        "    predictions: The quartiles predicted by  ( N x 3 x num_targets,)\n",
        "    k: constant , used to adjust the magnitude of the score. Default = 10\n",
        "    \n",
        "    \"\"\"\n",
        "    targets = targets.flatten()\n",
        "    predictions = predictions.flatten()\n",
        "    scaled_x = targets/targets\n",
        "    scaled_x_hat = predictions/targets\n",
        "    score= k*(10-np.sqrt(((scaled_x - scaled_x_hat) ** 2).mean()))\n",
        "    print(\"score is:\",score)\n",
        "    return score\n",
        "\n",
        "def load_Quartile_Table(path, order= None):\n",
        "    \"\"\"Read quartiles information from Quartiles Table and generate a 3D matrix \n",
        "    Args:\n",
        "        path (string): path to quartiles table\n",
        "        order (list, optional): order of the parameters, there is a default order if order is not given Defaults to None.\n",
        "    Returns:\n",
        "        _type_: quartiles matrix used for calculating the light track metric (N, 3, num_targets)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    quartiles = pd.read_csv(path)\n",
        "    if order is None:\n",
        "        targets = ['T','log_H2O', 'log_CO2','log_CH4','log_CO','log_NH3']\n",
        "    else:\n",
        "        targets = order\n",
        "    quartiles_matrix =  np.zeros((len(quartiles), 3, len(targets)))\n",
        "    for t_idx, t in enumerate(targets):\n",
        "        for q_idx, q in enumerate(['q1','q2','q3']):\n",
        "            quartiles_matrix[:,q_idx, t_idx, ] = quartiles.loc[:,t + '_' + q]\n",
        "    return quartiles_matrix\n",
        "\n",
        "def get_all_q(df):\n",
        "    \"\"\"get the GT values from the test set\"\"\"\n",
        "    all_q_label = ['q1','q2','q3']\n",
        "    all_q = np.zeros((3, len(df), 6))\n",
        "    for idx, label in enumerate(all_q_label):\n",
        "        q_matrix = df.loc[:, df.columns.str.endswith(label)].values\n",
        "        all_q[idx] = q_matrix\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9yWFzRBe3pW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "-----Main Functions----\n",
        "W2 - Wessestein-2 distance (implemented by POT package https://pythonot.github.io/)\n",
        "\"\"\"\n",
        "def batch_calculate(trace1_matrix, trace1_weights_matrix, trace2_hdf5, id_order = None):\n",
        "    \"\"\"Calculate the score for regular track from a (predicted) trace matrix and a Gound Truth hdf5 file\n",
        "    Args:\n",
        "        trace1_matrix (_type_): prediction from the model (N X M X 6), assumed to have the same number of trace for every examples\n",
        "        trace1_weights_matrix: Weight for trace1 matrix (N X M), where N is the number of examples and M is the number of traces. Each row should sum to 1.\n",
        "        trace2_hdf5 (_type_): GT trace data from a hdf5 file (do not need to open)\n",
        "        id_order (arr, optional): Order of the planets (N). Defaults to None.\n",
        "    Returns:\n",
        "        float: score for regular track\n",
        "    \"\"\"\n",
        "    # hdf5 requires knowledge on the order of the planets (in terms of planet id)\n",
        "    # If unspecified, it will assume ascending order from 1 to N. \n",
        "    trace2 = h5py.File(trace2_hdf5,'r')\n",
        "    if id_order is None:\n",
        "        id_order = np.arange(len(trace1_matrix))\n",
        "    else:\n",
        "        pass\n",
        "    # check size of the distribution \n",
        "    size = trace1_weights_matrix.shape[1]\n",
        "    if (size <1000) or (size > 5000):\n",
        "        print(\"Distribution size must be within 1000 to 5000\")\n",
        "        return None\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    all_score = 0\n",
        "    # ID_order = aux_test_data['planet_ID'].to_numpy()\n",
        "    for i, val in tqdm(enumerate(id_order)):\n",
        "        samples1 = trace1_matrix[i]\n",
        "        samples1_weight = trace1_weights_matrix[i]\n",
        "        samples2 = trace2[f'Planet_{val}']['tracedata'][:]\n",
        "        samples2_weight = trace2[f'Planet_{val}']['weights'][:]\n",
        "        # we assumed sample1 is from the participants and sample2 is from the GT.\n",
        "        ResampledUserTrace= nestle.resample_equal(samples1,samples1_weight)\n",
        "        ResampledUserWeights = np.ones(len(ResampledUserTrace))/len(ResampledUserTrace)\n",
        "        one_score = calculate_w2(ResampledUserTrace, samples2,w1=ResampledUserWeights,w2=samples2_weight,normalise=True)\n",
        "        all_score += one_score\n",
        "    overall_mean_score = all_score/len(trace1_matrix)\n",
        "    print(\"score is:\",overall_mean_score)\n",
        "    return overall_mean_score\n",
        "\n",
        "\n",
        "def batch_calculate_from_file(trace1_hdf5, trace2_hdf5):\n",
        "    \"\"\"calculate score from two .hdf5 files.\"\"\"\n",
        "    # read data from hdf5\n",
        "    trace1 = h5py.File(trace1_hdf5,'r')\n",
        "    trace2 = h5py.File(trace2_hdf5,'r')\n",
        "    trace1_keys = [p for p in trace1.keys()]\n",
        "    # assume trace1 is coming from participants\n",
        "    check_distribution(trace1, trace1_keys)\n",
        "\n",
        "    all_score = 0\n",
        "    # ID_order = aux_test_data['planet_ID'].to_numpy()\n",
        "    for val in tqdm(trace1_keys, total=len(trace1_keys)):\n",
        "        samples1 = trace1[val]['tracedata'][:]\n",
        "        samples1_weight = trace1[val]['weights'][:]\n",
        "        samples2 = trace2[val]['tracedata'][:]\n",
        "        samples2_weight = trace2[val]['weights'][:]\n",
        "        # we assumed sample1 is from the participants and sample2 is from the GT.\n",
        "        ResampledUserTrace= nestle.resample_equal(samples1,samples1_weight)\n",
        "        ResampledUserWeights = np.ones(len(ResampledUserTrace))/len(ResampledUserTrace)\n",
        "        one_score = calculate_w2(ResampledUserTrace, samples2,w1=ResampledUserWeights,w2=samples2_weight,normalise=True)\n",
        "        all_score += one_score\n",
        "    overall_mean_score = all_score/len(trace1_keys)\n",
        "    print(\"score is:\",overall_mean_score)\n",
        "    return overall_mean_score\n",
        "\n",
        "\n",
        "def calculate_w2(trace1, trace2, w1=None, w2=None, normalise = True, bounds_matrix = None):\n",
        "    \"\"\"Calculate the Wessestein-2 distnace metric between two multivariate distributions\n",
        "    Args:\n",
        "        trace1 (array):  N x D Matrix, where N is the number of points and D is the dimensionality\n",
        "        trace2 (array):  N x D Matrix, where N is the number of points and D is the dimensionality\n",
        "        w1 (array, optional):  1D Array of N length. Defaults to None.\n",
        "        w2 (array, optional):  1D Array of N length. Defaults to None.\n",
        "        normalise (bool, optional): _description_. Defaults to True.\n",
        "    Returns:\n",
        "        scalar: W2 distance between two empirical probability distribution\n",
        "    \"\"\" \n",
        "    \n",
        "    if normalise:\n",
        "        if bounds_matrix is None:\n",
        "            bounds_matrix = default_prior_bounds()\n",
        "        trace1 = normalise_arr(trace1,bounds_matrix )\n",
        "        trace2 = normalise_arr(trace2,bounds_matrix )\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    # calculate cost matrix\n",
        "    M = ot.dist(trace1, trace2)\n",
        "\n",
        "    # assume uniform weight if weights are not given\n",
        "    if w1 is None:\n",
        "        a = ot.unif(len(trace1))\n",
        "    else:\n",
        "        assert np.isclose(np.sum(w1),1)\n",
        "        a = w1\n",
        "    if w2 is None:\n",
        "        b = ot.unif(len(trace2))\n",
        "    else:\n",
        "        assert np.isclose(np.sum(w2),1)\n",
        "        b = w2\n",
        "    M /= M.max()\n",
        "    # numItermax controls the Max number of iteration before the solver \"gives up\" and return the result, \n",
        "    # recommended to use at least 100000 iterations for good results\n",
        "    W2 = ot.emd2(a, b, M, numItermax=100000)\n",
        "    # turn into an increasing score\n",
        "    score = 1000* (1-W2)\n",
        "    return score\n",
        "\n",
        "\"\"\"General Helper Functions\"\"\"\n",
        "\n",
        "def default_prior_bounds():\n",
        "    T_range = [0,7000]\n",
        "    gas1_range = [-12, -1]\n",
        "    gas2_range = [-12, -1]\n",
        "    gas3_range = [-12, -1]\n",
        "    gas4_range = [-12, -1]\n",
        "    gas5_range = [-12, -1]\n",
        "    bounds_matrix = np.vstack([T_range,gas1_range,gas2_range,gas3_range,gas4_range,gas5_range])\n",
        "    return bounds_matrix\n",
        "\n",
        "def normalise_arr(arr, bounds_matrix):\n",
        "    norm_arr = (arr - bounds_matrix[:,0])/(bounds_matrix[:,1]- bounds_matrix[:,0])\n",
        "    norm_arr = restrict_to_prior(norm_arr)\n",
        "    return norm_arr\n",
        "\n",
        "def restrict_to_prior(arr,):\n",
        "    # updates 06-oct, restrict values to default prior range \n",
        "    arr[arr<0] = 0\n",
        "    arr[arr>1] = 1\n",
        "    return arr\n",
        "\n",
        "def check_distribution(File, UserPlanets):\n",
        "    # updates from 06-Oct, perform various checks on the file, some of these changes are noted in submit_format.py from the baseline\n",
        "    for planet in UserPlanets:\n",
        "        traces = File[planet]['tracedata']\n",
        "        weights = File[planet]['weights']\n",
        "         ## sanity check - samples count should be the same for both\n",
        "        assert len(traces) == len(weights)\n",
        "        ## sanity check - weights must be able to sum to one.\n",
        "        assert np.isclose(np.sum(weights),1)\n",
        "        ## size of distribution must be between 1000 to 5000\n",
        "        assert ((len(weights) > 1000) & (len(weights)<5000))\n",
        "        ## weights must not be negatively negative\n",
        "        assert np.sum(weights<0) == 0 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3_EDDK7UAF-"
      },
      "outputs": [],
      "source": [
        "def to_light_track_format(q1_array, q2_array, q3_array, columns = None, name=\"LT_submission.csv\"):\n",
        "    \"\"\"Helper function to prepare submission file for the light track, \n",
        "    we assume the test data is arranged in assending order of the planet ID.\n",
        "    Args:\n",
        "        q1_array: N x 6 array containing the estimates for 16% percentile\n",
        "        q2_array: N x 6 array containing the estimates for 50% percentile\n",
        "        q3_array: N x 6 array containing the estimates for 84% percentile\n",
        "        columns: columns for the df. default to none\n",
        "    Returns:\n",
        "        Pandas DataFrame object\n",
        "    \"\"\"\n",
        "    # create empty array\n",
        "    LT_submission_df = pd.DataFrame(columns= columns)\n",
        "    # sanity check - length should be equal\n",
        "    assert len(q1_array) == len(q2_array) == len(q3_array)\n",
        "    targets_label = ['T', 'log_H2O', 'log_CO2','log_CH4','log_CO','log_NH3']\n",
        "    # create columns for df\n",
        "    default_quartiles = ['q1','q2','q3']\n",
        "    default_columns = []\n",
        "    for c in targets_label:\n",
        "        for q in default_quartiles:\n",
        "            default_columns.append(c+q)\n",
        "    \n",
        "    if columns is None:\n",
        "        columns = default_columns\n",
        "    for i in tqdm(range(len(q1_array))):\n",
        "        quartiles_dict = {}\n",
        "        quartiles_dict['planet_ID'] = i\n",
        "        for t_idx, t in enumerate(targets_label):\n",
        "            quartiles_dict[f'{t}_q1']= q1_array[i, t_idx]\n",
        "            quartiles_dict[f'{t}_q2']= q2_array[i, t_idx]\n",
        "            quartiles_dict[f'{t}_q3']= q3_array[i, t_idx]\n",
        "        LT_submission_df = pd.concat([LT_submission_df, pd.DataFrame.from_records([quartiles_dict])],axis=0,ignore_index = True)\n",
        "    LT_submission_df.to_csv(name,index= False)\n",
        "    return LT_submission_df\n",
        "\n",
        "\n",
        "def to_regular_track_format(tracedata_arr, weights_arr, name=\"RT_submission.hdf5\"):\n",
        "    \"\"\"convert input into regular track format.\n",
        "    we assume the test data is arranged in assending order of the planet ID.\n",
        "    Args:\n",
        "        tracedata_arr (array): Tracedata array, usually in the form of N x M x 6, where M is the number of tracedata, here we assume tracedata is of equal size. It does not have to be but you will need to craete an alternative function if the size is different. \n",
        "        weights_arr (array): Weights array, usually in the form of N x M, here we assumed the number of weights is of equal size, it should have the same size as the tracedata\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    submit_file = name\n",
        "    RT_submission = h5py.File(submit_file,'w')\n",
        "    for n in range(len(tracedata_arr)):\n",
        "        ## sanity check - samples count should be the same for both\n",
        "        assert len(tracedata_arr[n]) == len(weights_arr[n])\n",
        "        ## sanity check - weights must be able to sum to one.\n",
        "        assert np.isclose(np.sum(weights_arr[n]),1)\n",
        "\n",
        "        grp = RT_submission.create_group(f\"Planet_{n}\")\n",
        "        pl_id = grp.attrs['ID'] = n \n",
        "        tracedata = grp.create_dataset('tracedata',data=tracedata_arr[n])         \n",
        "        weight_adjusted = weights_arr[n]\n",
        "\n",
        "        weights = grp.create_dataset('weights',data=weight_adjusted)\n",
        "    RT_submission.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjiNxmfelCKm"
      },
      "source": [
        "*My function*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhy8gUO3gSzg"
      },
      "outputs": [],
      "source": [
        "#Get N spectra elements\n",
        "def get_spectra_noise(spec_matrix, N):\n",
        "  spectra = spec_matrix[:N,:,1]\n",
        "  wl_channels = len(spec_matrix[0,:,0])\n",
        "  noise = spec_matrix[:N,:,2]\n",
        "\n",
        "  global_mean = np.mean(spectra)\n",
        "  global_std = np.std(spectra)\n",
        "\n",
        "  global_max = np.max(spectra)\n",
        "  global_min = np.min(spectra)\n",
        "  # return spectra and noise\n",
        "  return spectra, noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyoqYjlvxXui"
      },
      "outputs": [],
      "source": [
        "#Get N rows of aux table\n",
        "def get_add_info(aux_training_data, N):\n",
        "  radii = aux_training_data.iloc[:N, :].copy()\n",
        "  mean_radii = radii.mean()\n",
        "  std_radii = radii.std()\n",
        "  # Features engineering\n",
        "  radii[\"s\"] = ((radii[\"star_mass_kg\"]*radii[\"star_radius_m\"]/radii[\"star_temperature\"])/radii[\"planet_distance\"])\n",
        "  radii[\"a\"] = radii[\"s\"]/radii[\"planet_surface_gravity\"]\n",
        "  radii[\"c\"] = ((radii[\"star_mass_kg\"]*radii[\"star_radius_m\"]**2/radii[\"star_temperature\"])/(radii[\"planet_distance\"]))\n",
        "  radii[\"d\"] = (  (4/3*math.pi*(radii[\"star_radius_m\"]**3))-(4/3*math.pi*(radii[\"planet_radius_m\"]**3)))\n",
        "  radii[\"f\"] = (radii[\"star_mass_kg\"]/radii[\"planet_mass_kg\"])\n",
        "  radii[\"e\"] = radii[\"star_temperature\"]*radii[\"planet_distance\"]/radii[\"planet_orbital_period\"]\n",
        "  radii[\"g\"] = radii[\"planet_surface_gravity\"]/radii[\"planet_mass_kg\"]\n",
        "  radii[\"gg\"] = (radii[\"star_mass_kg\"]/radii[\"g\"])\n",
        "  radii[\"Luminosity\"] = 4*scipy.constants.pi*scipy.constants.Stefan_Boltzmann*(radii[\"star_radius_m\"]**2)*(radii[\"star_temperature\"]**4)\n",
        "  # Delete ID\n",
        "  del radii[\"planet_ID\"]\n",
        "  max_radii = radii.max()\n",
        "  min_radii = radii.min()\n",
        "  # Black body\n",
        "  radii[\"Temp_black\"]= ((radii[\"Luminosity\"]*1)/(scipy.constants.pi*scipy.constants.Stefan_Boltzmann*16*(radii[\"planet_radius_m\"]**2)))**(1/4)*(radii[\"planet_distance\"]**(-(1/2)))\n",
        "  radii[\"Temp_black1\"]= (1.0-0.0)**(1/4)*(radii[\"planet_distance\"]**(-(1/2)))*279\n",
        "  return radii"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJu9RwMKVnIL"
      },
      "outputs": [],
      "source": [
        "# Augmentation data\n",
        "def augment_data_with_noise(spectra, noise, repeat ):\n",
        "    aug_spectra, aug_noise = augment_data(spectra, noise, repeat)\n",
        "    return aug_spectra, aug_noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujqw2zyXRiK0"
      },
      "outputs": [],
      "source": [
        "# Standardize\n",
        "def standardize(arr, noise, global_mean, global_std):\n",
        "    for i in tqdm(range(0,len(arr))):      \n",
        "      arr[i] = (arr[i]-global_mean)/global_std\n",
        "      # Normalize between -1 and 1\n",
        "      noise[i] =  (noise[i]-np.mean(noise[i]))/np.std(noise[i]) \n",
        "      noise[i] = (noise[i]-np.min(noise[i]))/(np.max(noise[i])-np.min(noise[i])) \n",
        "      noise[i] = (noise[i]*2)-1\n",
        "    return arr, noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmBXrKGSAGi6"
      },
      "outputs": [],
      "source": [
        "#Standard Normal Variate\n",
        "def snv(input_data):\n",
        "    # Define a new array and populate it with the corrected data  \n",
        "    data_snv = np.zeros_like(input_data)\n",
        "    for i in range(data_snv.shape[0]):\n",
        "    # Apply correction\n",
        "      data_snv[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / np.std(input_data[i,:])\n",
        "    return (data_snv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Ec2EaOlf7D"
      },
      "outputs": [],
      "source": [
        "# Augmentation data\n",
        "def aug_data(training_spectra, training_noise, repeat, global_mean, global_std):\n",
        "  # Augmentation \n",
        "  training_spectra, training_profile_noise = augment_data_with_noise(training_spectra, training_noise, repeat)\n",
        "  training_spectra = np.swapaxes(training_spectra, 0, 1)\n",
        "  # Apply SNV\n",
        "  training_spectra = np.asarray( [ snv(t) for t in tqdm(training_spectra)] ) \n",
        "  print(training_spectra.shape)\n",
        "  training_spectra = training_spectra.reshape(-1, spectra.shape[1])\n",
        "  # Standardise\n",
        "  # training_noise is training_spectra but normalized between -1 and 1\n",
        "  training_spectra, training_noise = standardize(training_spectra.copy(), training_spectra.copy(), global_mean, global_std)\n",
        "  training_spectra = training_spectra.reshape(-1, repeat, spectra.shape[1])\n",
        "  training_noise = training_noise.reshape(-1, repeat, spectra.shape[1])\n",
        "  print(training_spectra.shape) \n",
        "  # return spectra and another kind of spectra\n",
        "  return training_spectra, training_noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLm9BU16KOz"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load data*"
      ],
      "metadata": {
        "id": "Z3rMjyNgDiUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW5lftRY6NXP"
      },
      "outputs": [],
      "source": [
        "# Set path\n",
        "training_path = 'Public/TrainingData/'\n",
        "test_path = './final_evaluation_set' \n",
        "training_GT_path = os.path.join(training_path, 'Ground Truth Package')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v0XXNUiUWQh"
      },
      "outputs": [],
      "source": [
        "# Get hdf5 file\n",
        "spectral_training_data = h5py.File(os.path.join(training_path,'SpectralData.hdf5'),\"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbm5iiAWz9lA"
      },
      "outputs": [],
      "source": [
        "# Get Auxillary table\n",
        "aux_training_data = pd.read_csv(os.path.join(training_path,'AuxillaryTable.csv'))\n",
        "soft_label_data = pd.read_csv(os.path.join(training_GT_path, 'FM_Parameter_Table.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecgY8j_QeELQ"
      },
      "outputs": [],
      "source": [
        "# Get spectra\n",
        "spec_matrix = to_observed_matrix(spectral_training_data,aux_training_data)\n",
        "print(\"spectral matrix shape:\", spec_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28oi4B6Rq0RJ"
      },
      "outputs": [],
      "source": [
        "# Get ground truth\n",
        "GT_Quartiles_path = os.path.join(training_path+\"Ground Truth Package/\", 'QuartilesTable.csv')\n",
        "all_qs_dataset = load_Quartile_Table(GT_Quartiles_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbyctUvJxjZz"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgTMV-aExjJB"
      },
      "outputs": [],
      "source": [
        "N = 10000\n",
        "threshold = 0.95 \n",
        "repeat = 350 # ( 3 * repeat ) > 1000 with the new rules I have to generate more than 1000 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get target"
      ],
      "metadata": {
        "id": "5WC1cBQpENYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifDozfLBd9lt"
      },
      "outputs": [],
      "source": [
        "all_qs = all_qs_dataset[:N, :, 0:].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaC7El-QxfIa"
      },
      "source": [
        "Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6GZ6E3JxgIQ"
      },
      "outputs": [],
      "source": [
        "target_labels = ['planet_temp','log_H2O','log_CO2','log_CH4','log_CO','log_NH3']\n",
        "\n",
        "targets = soft_label_data.iloc[:N][target_labels]\n",
        "num_targets = targets.shape[1]\n",
        "targets_mean = targets.mean()\n",
        "targets_std = targets.std()\n",
        "\n",
        "targets_max = targets.max()\n",
        "targets_min = targets.min()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "F3WC3QK9EYeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACORriXjxp4R"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "# Get spectra\n",
        "spectra, noise = get_spectra_noise(spec_matrix[:N], N)\n",
        "radii = get_add_info(aux_training_data[:N], N)\n",
        "# Index for split train/validation\n",
        "ind = np.random.rand(len(spectra)) < threshold\n",
        "# Split train/validation\n",
        "training_spectra, training_radii, training_targets, training_noise  = spectra[ind].copy(),radii[ind].copy(),all_qs[ind].copy(), noise[ind].copy()\n",
        "valid_spectra, valid_radii, valid_targets, valid_noise, = spectra[~ind].copy(),radii[~ind].copy(),all_qs[~ind].copy(), noise[~ind].copy()\n",
        "print(training_targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Preprocessing*"
      ],
      "metadata": {
        "id": "QjpOF3UHEx_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo7PIMtdVZYd"
      },
      "outputs": [],
      "source": [
        "global_mean = np.mean(training_spectra)\n",
        "global_std = np.std(training_spectra)\n",
        "# Preprocessing traning set\n",
        "training_spectra, training_noise = aug_data(training_spectra, training_noise, repeat, global_mean, global_std)\n",
        "# Preprocessing validation set\n",
        "valid_spectra, valid_noise = aug_data(valid_spectra, valid_noise, repeat, global_mean, global_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Plot spectra*"
      ],
      "metadata": {
        "id": "Ns4TU2ETFAnD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btAJjcd_RuZG"
      },
      "outputs": [],
      "source": [
        "x_axes = [i for i in range(52)]\n",
        "for tt in training_noise[5]:\n",
        "  plt.plot(x_axes, tt)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Shape*"
      ],
      "metadata": {
        "id": "iUlcRGbUFMVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vj9r8X9skfc"
      },
      "outputs": [],
      "source": [
        "print(training_noise.shape, training_radii.shape, training_targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1yUkVfzx6dB"
      },
      "source": [
        "# Network temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIzu4HetUN76"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)\n",
        "from keras import initializers\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Reshape, Input, Concatenate, BatchNormalization, Dropout, Conv1D,Flatten,MaxPooling1D, Conv2D, MaxPooling2D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Task Metric*"
      ],
      "metadata": {
        "id": "7BWk9KhPFV7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjxBbuVadQoT"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K \n",
        "# https://www.ariel-datachallenge.space/ML/documentation/scoring Light Track metric\n",
        "def rmse(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square((y_true-y_pred)/y_true)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Scale aux table*"
      ],
      "metadata": {
        "id": "J6UsscaIFd_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate scaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "# Scale training set\n",
        "x_scaled1 = min_max_scaler.fit_transform(training_radii.copy()) \n",
        "df_data = pd.DataFrame(x_scaled1)\n",
        "# Scale validation set\n",
        "x_scaled2 = min_max_scaler.transform(valid_radii.copy())\n",
        "df_data_test = pd.DataFrame(x_scaled2)"
      ],
      "metadata": {
        "id": "NITLNugcFdz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Target values temperature*"
      ],
      "metadata": {
        "id": "VYMripg4FiXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = training_targets[:,:,0].copy() \n",
        "y_val = valid_targets[:,:,0].copy() "
      ],
      "metadata": {
        "id": "o79hrijzFl5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model temperature**"
      ],
      "metadata": {
        "id": "rbdmkhOaFmLF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P81HBPZN6YbT"
      },
      "outputs": [],
      "source": [
        "# Repeat, number of M repetitions per spectrum\n",
        "def temp_model(repeat):\n",
        "    # Aux table\n",
        "    InputLayer1 = Input(shape=(20,))\n",
        "    dd = Dense(256, activation=\"relu\")(InputLayer1)\n",
        "    dd = Dense(128, activation=\"relu\")(dd)\n",
        "    dd = Dense(64, activation=\"relu\")(dd)\n",
        "    dd = Dense(32, activation=\"relu\")(dd)\n",
        "    dd = Dense(16, activation=\"relu\")(dd)\n",
        "    dd = Dense(8, activation= 'relu')(dd)\n",
        "    # Spectrum\n",
        "    inputs = keras.layers.Input(shape=(repeat, 52), name=\"input\")\n",
        "    x = Reshape((repeat, 52, 1))(inputs)\n",
        "    x = Conv2D(8, (1,3), activation='relu')(x)\n",
        "    x = Conv2D(8, (1,3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(8, (1,3), activation='relu')(x)\n",
        "    x = Conv2D(8, (1,3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Dropout(0.25)(x,training=True)\n",
        "    x = Dense(128, activation= 'relu')(x)\n",
        "    x = Dense(32, activation= 'relu')(x)\n",
        "    x = Dense(8, activation= 'relu')(x)\n",
        "    x = Reshape((repeat, -1))(x)\n",
        "    feat = tf.keras.layers.RepeatVector(repeat)(dd)\n",
        "    # Merge two data\n",
        "    x = tf.keras.layers.Concatenate()([x, feat])\n",
        "    # BATCH | REPEAT | (16th, 50th, 84th) \n",
        "    x = keras.layers.Dense(3)(x)\n",
        "    # Real output\n",
        "    x = Reshape((repeat, 3, 1), name=\"real_output\")(x)\n",
        "    # Average on real output\n",
        "    outputs = keras.layers.AveragePooling2D(pool_size=(repeat,1), strides=(1,1))(x)\n",
        "    outputs = Reshape((3, 1))(outputs)\n",
        "\n",
        "    return keras.models.Model(inputs=[inputs, InputLayer1], outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Print model*"
      ],
      "metadata": {
        "id": "wAYuu2LfGb29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBAvyp865t0l"
      },
      "outputs": [],
      "source": [
        "tmod = temp_model(repeat)\n",
        "tmod.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train temperature**"
      ],
      "metadata": {
        "id": "yl-l0esSGfzU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_3X-g5YtF8j"
      },
      "outputs": [],
      "source": [
        "tmod.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    metrics=  tf.keras.metrics.MeanAbsolutePercentageError(),\n",
        "    loss = rmse\n",
        "    )\n",
        "\n",
        "tmod_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"drive/My Drive/ESA/temp_check1.ckp\",\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor='val_mean_absolute_percentage_error',\n",
        "    mode='min')\n",
        "\n",
        "tmod.fit([training_noise, df_data ], [y], \n",
        "          validation_data=([valid_noise, df_data_test ], [y_val]),\n",
        "          batch_size=100, \n",
        "          callbacks=[tmod_checkpoint_callback],\n",
        "          epochs=100, \n",
        "          shuffle=False,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Save model*"
      ],
      "metadata": {
        "id": "1bcxwHWsHHVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZV2Zc5u30yw"
      },
      "outputs": [],
      "source": [
        "tmod.load_weights(\"drive/My Drive/ESA/temp_check1.ckp\")\n",
        "tmod.save(\"drive/My Drive/ESA/model_t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Check metric*"
      ],
      "metadata": {
        "id": "t-jOtYQZHKHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rJlo_QApFKj"
      },
      "outputs": [],
      "source": [
        "# Check on the average value\n",
        "s_ = tmod.predict([training_noise, df_data])[:,1]\n",
        "print(mean_squared_error(training_targets[:,1,0], s_))\n",
        "s_ = tmod.predict([valid_noise, df_data_test])[:,1]\n",
        "print(mean_squared_error(valid_targets[:,1,0], s_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLV1T4HxcqUO"
      },
      "source": [
        "# Network chemicals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7a3WMXcWSPl"
      },
      "outputs": [],
      "source": [
        "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
        "    # residual block\n",
        "    s = keras.layers.Conv2D(filters, 1, padding=\"same\")(x)\n",
        "    for i in range(conv_num - 1):\n",
        "        x = keras.layers.Conv2D (filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.Activation(activation)(x)\n",
        "    x = keras.layers.Conv2D (filters, 3, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.SpatialDropout2D(0.25)(x)\n",
        "    x = keras.layers.Add()([x, s])\n",
        "    x = keras.layers.Activation(activation)(x)\n",
        "    return keras.layers.MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "\n",
        "\n",
        "def chemicals_model():\n",
        "    # Spectrum\n",
        "    inputs = keras.layers.Input(shape=(repeat, 52), name=\"input\")\n",
        "    x = Reshape((repeat, 52, 1))(inputs)\n",
        "    x = residual_block(x, 32, 2)\n",
        "    x = residual_block(x, 64, 2)\n",
        "    x = residual_block(x, 128, 3)\n",
        "    # Attention\n",
        "    x1 = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=1)(inputs,inputs)\n",
        "    x1 = Dense(768, activation=\"relu\")(x1)\n",
        "    # BATCH | Repeat | ?\n",
        "    x = Reshape((repeat, -1))(x)\n",
        "    x = tf.keras.layers.Add()([x1, x])\n",
        "    # Aux table\n",
        "    feat_input = Input(shape=(10,))\n",
        "    feat = tf.keras.layers.RepeatVector(repeat)(feat_input)\n",
        "    # Merge\n",
        "    x = tf.keras.layers.Concatenate()([x, feat])\n",
        "    x = keras.layers.Dense(3*5)(x)\n",
        "    # BATCH | Repeat | (16th, 50th, 84th) | 5 chemicals\n",
        "    x = Reshape((repeat, 3, 5), name=\"real_output\")(x)\n",
        "    # Average\n",
        "    outputs = keras.layers.AveragePooling2D(pool_size=(repeat,1), strides=(1,1))(x)\n",
        "    outputs = Reshape((3, 5))(outputs)\n",
        "    return keras.models.Model(inputs=[inputs, feat_input], outputs=outputs)\n",
        "\n",
        "model = chemicals_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model*"
      ],
      "metadata": {
        "id": "pQpiIX4hI0G5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1-4NRyPgenS"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyVG8Ev3yBHw"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGGprYKvyHaZ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    metrics= [tf.keras.metrics.MeanAbsolutePercentageError()],\n",
        "    loss = rmse\n",
        "    )\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"drive/My Drive/ESA/model_check2.ckp\",\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor='val_mean_absolute_percentage_error',\n",
        "    mode='min')\n",
        "\n",
        "model.fit([training_noise, df_data[df_data.columns[:10]]], \n",
        "          [training_targets[:,:,1:]],\n",
        "          validation_data=([valid_noise, df_data_test[df_data_test.columns[:10]]], \n",
        "                           [valid_targets[:,:,1:]]),\n",
        "          batch_size=100, \n",
        "          callbacks=[model_checkpoint_callback],\n",
        "          epochs=60,\n",
        "          shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Save model*"
      ],
      "metadata": {
        "id": "8X-uK7_eI45O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQdLH8AsAxWN"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"drive/My Drive/ESA/model_check2.ckp\")\n",
        "model.save('drive/My Drive/ESA/model_c')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and print the model"
      ],
      "metadata": {
        "id": "YBajichp5JAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmod = tf.keras.models.load_model(\"drive/My Drive/ESA/model_t\", custom_objects={'rmse': rmse}) \n",
        "model = tf.keras.models.load_model('drive/My Drive/ESA/model_c', custom_objects={'rmse': rmse}) "
      ],
      "metadata": {
        "id": "F6sbTfZg54L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, to_file=\"chemicals.png\", \n",
        "    rankdir=\"LR\",\n",
        "    show_layer_names=False,\n",
        "    show_shapes=False)"
      ],
      "metadata": {
        "id": "UF3DGOdF54t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(tmod, to_file=\"temperature.png\", \n",
        "    rankdir=\"LR\",\n",
        "    show_layer_names=False,\n",
        "    show_shapes=False)"
      ],
      "metadata": {
        "id": "vnZ6yGNbpdZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM3MQc6KYBLm"
      },
      "source": [
        "# Test validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJq3rbEYX8pF"
      },
      "outputs": [],
      "source": [
        "## select the corresponding GT for the validation data, and in the correct order.\n",
        "index= np.arange(len(ind))\n",
        "valid_index = index[~ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRwtv3_WYAaM"
      },
      "outputs": [],
      "source": [
        "# Get real output from chemicals\n",
        "model_ = tf.keras.Model(inputs=model.input, outputs=model.get_layer(\"real_output\").output)\n",
        "# Get real output from temperature\n",
        "model_t = tf.keras.Model(inputs=tmod.input, outputs=tmod.get_layer(\"real_output\").output)\n",
        "\n",
        "# Predict chemicals\n",
        "y_pred = model_.predict([valid_noise, df_data_test[df_data_test.columns[:10]]])\n",
        "# Predict temperature\n",
        "y_pred_temp = model_t.predict([valid_noise, df_data_test])\n",
        "\n",
        "# Shape\n",
        "print(y_pred.shape)\n",
        "print(y_pred_temp.shape)\n",
        "\n",
        "# Average chemicals\n",
        "y_pred_valid = (y_pred)\n",
        "y_pred_valid = np.average(y_pred_valid, axis=1)\n",
        "print(y_pred_valid.shape)\n",
        "\n",
        "# Average temperature\n",
        "y_pred_temp_valid = (y_pred_temp)\n",
        "y_pred_temp_valid = np.average(y_pred_temp_valid, axis=1)\n",
        "\n",
        "# Concatenate temperature and chemicals\n",
        "y_pred_valid = np.concatenate((y_pred_temp_valid, y_pred_valid), axis=2)\n",
        "print(y_pred_valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Df5Euun9JIc"
      },
      "source": [
        "# Light evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY2S7IKotLiP"
      },
      "outputs": [],
      "source": [
        "### load the ground truth\n",
        "valid_GT_Quartiles = all_qs[valid_index]\n",
        "valid_GT_Quartiles = np.swapaxes(valid_GT_Quartiles, 1,0)[:,:,]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHr0nGO-us70"
      },
      "source": [
        "Quartile prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okx1qgs-hT6V"
      },
      "outputs": [],
      "source": [
        "print(y_pred.shape)\n",
        "# Concatenate real output (temperature and chemicals)\n",
        "valid_q_pred11 = np.concatenate((y_pred_temp, y_pred), axis=3)\n",
        "valid_q_pred11 = np.reshape(valid_q_pred11, (y_pred.shape[0], repeat*3, 6))\n",
        "valid_q_pred11.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGlTH74ONHJO"
      },
      "outputs": [],
      "source": [
        "# Get quartile\n",
        "valid_q_pred = np.quantile(y_pred_valid, [0.16,0.5,0.84],axis=1)\n",
        "valid_q_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJDwteziM8H3"
      },
      "outputs": [],
      "source": [
        "# calculate!\n",
        "light_track_metric(valid_GT_Quartiles, valid_q_pred, k =100) #993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGRS_bhjeYZ6"
      },
      "source": [
        "# Evaluate - regular track\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VF-juNkeZ3B"
      },
      "outputs": [],
      "source": [
        "## read trace and quartiles table \n",
        "GT_trace_path = os.path.join(training_GT_path, 'Tracedata.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj0XfJNZ1eIE"
      },
      "outputs": [],
      "source": [
        "print(y_pred.shape)\n",
        "# Concatenate real output (temperature and chemicals)\n",
        "valid_q_pred1 = np.concatenate((y_pred_temp, y_pred), axis=3)\n",
        "valid_q_pred1 = np.reshape(valid_q_pred1, (y_pred.shape[0], repeat*3, 6))\n",
        "valid_q_pred1.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates weights\n",
        "def generate_w(t):\n",
        "  a = np.reshape(t, -1)\n",
        "  a = np.sort(a)\n",
        "  split = len(a)//3\n",
        "  max_ = a[-split:]\n",
        "  np.random.shuffle(a[:-split])\n",
        "  a = np.reshape(a, (3,-1))\n",
        "  t = np.dstack((a[0],a[2], a[1]))\n",
        "  return t"
      ],
      "metadata": {
        "id": "MGC6tBihwRMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH4PBCbPegnE"
      },
      "outputs": [],
      "source": [
        "# Get real output\n",
        "trace1_matrix = valid_q_pred1 \n",
        "# Generate normal distribution\n",
        "norm = np.random.normal(loc=0.0, scale=1, size=(y_pred.shape[1], y_pred.shape[2]))\n",
        "# Edit the distribution\n",
        "norm = generate_w(norm)\n",
        "norm = np.reshape(norm, -1)\n",
        "# Only positive\n",
        "norm -= norm.min()\n",
        "# Sum 1\n",
        "sum_norm = sum(norm)\n",
        "norm = norm/sum_norm\n",
        "# Generate weights\n",
        "trace1_weights_matrix = np.asarray([norm for i in range(trace1_matrix.shape[0])])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate new batch\n",
        "batch_calculate(trace1_matrix, trace1_weights_matrix, GT_trace_path, id_order = valid_index) "
      ],
      "metadata": {
        "id": "ZhM6Wcb_eZNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFhFujesHNNM"
      },
      "source": [
        "# Generate Test File\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_path"
      ],
      "metadata": {
        "id": "ibi3XXAzA1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92VkjjVsHPll"
      },
      "outputs": [],
      "source": [
        "spec_test_data = h5py.File(os.path.join(test_path,'SpectralData.hdf5'),\"r\")\n",
        "aux_test_data = pd.read_csv(os.path.join(test_path,'AuxillaryTable.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qumwRhmYKwiQ"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_gnFYhxHU8z"
      },
      "outputs": [],
      "source": [
        "test_spec_matrix = to_observed_matrix(spec_test_data,aux_test_data )\n",
        "print(test_spec_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JdaPnEhKx0S"
      },
      "source": [
        "**Get data**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*real data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2Gh1p-hHvoZ"
      },
      "outputs": [],
      "source": [
        "spectra_test, noise_test = get_spectra_noise(test_spec_matrix, N)\n",
        "radii_columns = list(aux_test_data.columns)\n",
        "radii_columns[0] = \"planet_ID\"\n",
        "aux_test_data.columns = radii_columns\n",
        "radii_test = get_add_info(aux_test_data, N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzO4_Xm9F5sf"
      },
      "source": [
        "*supervisioned test it can no longer be used*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml0dpc_bFEGZ"
      },
      "outputs": [],
      "source": [
        "spectra_supertest, noise_supertest = get_spectra_noise(spec_matrix[N:21988], N)\n",
        "radii_supertest = get_add_info(aux_training_data[N:21988], N)\n",
        "spectra_supertest.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uX5Atl1KziU"
      },
      "source": [
        "**Preprocessing**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*real test*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6HNQWuKKeLT"
      },
      "outputs": [],
      "source": [
        "spectra_test, noise_test = aug_data(spectra_test, noise_test, repeat, global_mean, global_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgO5l3RvGRlq"
      },
      "source": [
        "*supervisioned test it can no longer be used*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeMEgiOfGTTB"
      },
      "outputs": [],
      "source": [
        "spectra_supertest, noise_supertest = aug_data(spectra_supertest, noise_supertest, repeat, global_mean, global_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETcFWyIGavh"
      },
      "source": [
        "**Scale aux table**\n",
        "\n",
        "\n",
        "---\n",
        "*real test*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "milly9jhLV15"
      },
      "outputs": [],
      "source": [
        "radii_test_scaled = min_max_scaler.transform(radii_test.copy())\n",
        "df_radii_test = pd.DataFrame(radii_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*supervisioned test it can no longer be used*"
      ],
      "metadata": {
        "id": "Wsk_v0Z4LzzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBeAQAZ1GdkR"
      },
      "outputs": [],
      "source": [
        "radii_supertest_scaled = min_max_scaler.transform(radii_supertest.copy())\n",
        "radii_supertest = pd.DataFrame(radii_supertest_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shape**"
      ],
      "metadata": {
        "id": "c6f1-jhGL3dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iTldoQONdLN"
      },
      "outputs": [],
      "source": [
        "print(spectra_test.shape)\n",
        "print(df_radii_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozYK8AsZMwh_"
      },
      "source": [
        "**Prediction**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*real test*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuH0mUyGMyJB"
      },
      "outputs": [],
      "source": [
        "# Model chemicals\n",
        "model_ = tf.keras.Model(inputs=model.input, outputs=model.get_layer(\"real_output\").output)\n",
        "# Model temperature\n",
        "model_t = tf.keras.Model(inputs=tmod.input, outputs=tmod.get_layer(\"real_output\").output)\n",
        "\n",
        "# Prediction chemicals\n",
        "y_pred_test = model_.predict([noise_test, df_radii_test[df_radii_test.columns[:10]]])\n",
        "# Prediction temperature\n",
        "y_pred_temp_test = model_t.predict([noise_test, df_radii_test]) \n",
        "\n",
        "# Shape\n",
        "print(y_pred_test.shape)\n",
        "print(y_pred_temp_test.shape)\n",
        "\n",
        "# Average chemicals\n",
        "y_pred_valid_test = y_pred_test\n",
        "y_pred_valid_test = np.average(y_pred_test, axis=1)\n",
        "# Average temperature\n",
        "y_pred_valid_temp_test = y_pred_temp_test\n",
        "y_pred_valid_temp_test = np.average(y_pred_valid_temp_test, axis=1)\n",
        "\n",
        "print(y_pred_valid_test.shape)\n",
        "# Concatenate temperature and chemicals\n",
        "y_pred_valid_test = np.concatenate((y_pred_valid_temp_test, y_pred_valid_test), axis=2)\n",
        "print(y_pred_valid_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6APuzhI-5J"
      },
      "source": [
        "# Quartiles Test file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG2NxothDM-V"
      },
      "outputs": [],
      "source": [
        "print(y_pred_test.shape)\n",
        "print(y_pred_temp_test.shape)\n",
        "# Concatenate temperature and chemicals\n",
        "q_test = np.concatenate((y_pred_temp_test, y_pred_test), axis=3)\n",
        "q_test = np.reshape(q_test, (y_pred_test.shape[0], repeat*3, 6))\n",
        "q_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzmbcs6WRmrM"
      },
      "source": [
        "**LT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5RLyIgdRoeT"
      },
      "outputs": [],
      "source": [
        "# Quartiles\n",
        "valid_q_pred = np.quantile(q_test, [0.16,0.5,0.84],axis=1)\n",
        "print(valid_q_pred.shape)\n",
        "# Generate LT\n",
        "LT_submission = to_light_track_format(valid_q_pred[0], valid_q_pred[1], valid_q_pred[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RyA9wBjSKc0"
      },
      "source": [
        "**RT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4r5uQAkSZEK"
      },
      "outputs": [],
      "source": [
        "print(y_pred_test.shape)\n",
        "# Concaneta real output temperature and chemicals\n",
        "valid_q_pred1_test = np.concatenate((y_pred_temp_test, y_pred_test), axis=3)\n",
        "valid_q_pred1_test = np.reshape(valid_q_pred1_test, (y_pred_test.shape[0], repeat*3, 6))\n",
        "valid_q_pred1_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgQpY7sjSNE1"
      },
      "outputs": [],
      "source": [
        "# Quartiles\n",
        "y_pred_org = valid_q_pred1_test\n",
        "# Generates weights\n",
        "norm = np.random.normal(loc=0.0, scale=1, size=(y_pred_test.shape[1], y_pred_test.shape[2]))\n",
        "# Edit the distribution\n",
        "norm = generate_w(norm)\n",
        "norm = np.reshape(norm, -1)\n",
        "# Only positive\n",
        "norm -= norm.min()\n",
        "# Sum 1\n",
        "sum_norm = sum(norm)\n",
        "norm = norm/sum_norm\n",
        "# Generate weights\n",
        "weight = np.asarray([norm for i in range(y_pred_org.shape[0])])\n",
        "print(weight.shape)\n",
        "# Generate RT\n",
        "RT_submission = to_regular_track_format(y_pred_org, \n",
        "                                        weight, \n",
        "                                        name=\"RT_submission.hdf5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYW2wMv0TAcZ"
      },
      "source": [
        "# Control of generated files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LT*"
      ],
      "metadata": {
        "id": "uF7_yrXCPi5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scpbPilWTCNX"
      },
      "outputs": [],
      "source": [
        "LL = pd.read_csv(\"LT_submission.csv\")\n",
        "LL.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*RT*"
      ],
      "metadata": {
        "id": "2Tfj1VeCPkgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-vIeA9pTDlz"
      },
      "outputs": [],
      "source": [
        "f = h5py.File(\"RT_submission.hdf5\",'r')\n",
        "f['Planet_0']['tracedata'][:]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d7k7QhHtqLVc",
        "a6FJxBmaW8tP",
        "JWgpEVrmqOtv",
        "OvLm9BU16KOz",
        "SbyctUvJxjZz",
        "U1yUkVfzx6dB",
        "zLV1T4HxcqUO",
        "YBajichp5JAM",
        "hM3MQc6KYBLm",
        "1Df5Euun9JIc",
        "ZGRS_bhjeYZ6",
        "ZFhFujesHNNM",
        "PU6APuzhI-5J",
        "rYW2wMv0TAcZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}